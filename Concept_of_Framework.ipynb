{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Pure Large Language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "def main():\n",
    "    model_name =\"DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name,torch_dtype = torch.float16,device_map = \"auto\")\n",
    "\n",
    "    def deepseek_generate(prompt):\n",
    "        inputs = tokenizer(prompt,return_tensors=\"pt\").to(\"cuda\")\n",
    "        # output = model.generate(**inputs,max_length =126)\n",
    "        output = model.generate(**inputs,max_new_tokens=1000)    \n",
    "        return tokenizer.decode(output[0],skip_special_tokens=False)\n",
    "\n",
    "    query = \"what is RAG in AI ?\"\n",
    "    print(deepseek_generate(query))\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "if __name__ ==\"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "import os\n",
    "import PyPDF2\n",
    "import docx\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extract text from a PDF file.\"\"\"\n",
    "    text = \"\"\n",
    "    with open(pdf_path, \"rb\") as file:\n",
    "        pdf_reader = PyPDF2.PdfReader(file)\n",
    "        for page in pdf_reader.pages:\n",
    "            text += page.extract_text() + \"\\n\"\n",
    "    return text\n",
    "\n",
    "def extract_text_from_word(docx_path):\n",
    "    \"\"\"Extract text from a Word file.\"\"\"\n",
    "    doc = docx.Document(docx_path)\n",
    "    text = \"\\n\".join([para.text for para in doc.paragraphs])\n",
    "    return text\n",
    "\n",
    "def load_documents_from_folder(folder_path):\n",
    "    \"\"\"Read all PDF and Word files from a folder and extract text.\"\"\"\n",
    "    documents = []\n",
    "    \n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "        if filename.endswith(\".pdf\"):\n",
    "            text = extract_text_from_pdf(file_path)\n",
    "        elif filename.endswith(\".docx\"):\n",
    "            text = extract_text_from_word(file_path)\n",
    "        else:\n",
    "            continue  # Skip non-PDF and non-Word files\n",
    "\n",
    "        documents.append({\"filename\": filename, \"content\": text})\n",
    "    \n",
    "    return documents\n",
    "\n",
    "def main(): \n",
    "\n",
    "    embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "\n",
    "    # Define text splitter (for chunking long documents)\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=100)\n",
    "\n",
    "    # Load documents from a folder\n",
    "    folder_path = \"RAG Information\"  # Replace with your folder path\n",
    "    documents = load_documents_from_folder(folder_path)\n",
    "\n",
    "    # Process and convert text into embeddings\n",
    "    all_docs = []\n",
    "    for doc in documents:\n",
    "        split_docs = text_splitter.create_documents([doc[\"content\"]])\n",
    "        all_docs.extend(split_docs)\n",
    "\n",
    "    # Create FAISS vector database\n",
    "    vectorstore = FAISS.from_documents(all_docs, embedding_model)\n",
    "\n",
    "    # Save FAISS index\n",
    "    vectorstore.save_local(\"faiss_index\")\n",
    "\n",
    "if __name__ ==\"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo using Large Language Model with RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch \n",
    "from torch import autocast\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "model_name =\"DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,torch_dtype = torch.float16,device_map = \"auto\")\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "vectorstore = FAISS.load_local(\"faiss_index\", embedding_model,allow_dangerous_deserialization=True)\n",
    "\n",
    "def deepseek_generate(prompt):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model.to(device)\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    with autocast(device_type=device, dtype=torch.float16):  # Use mixed precision\n",
    "        output = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=1000\n",
    "        )\n",
    "\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "def retrieve_context(query):\n",
    "    \"\"\"Retrieve the most relevant document chunks for a given query.\"\"\"\n",
    "    retrieved_docs = vectorstore.similarity_search(query, k=3)\n",
    "    return \"\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "\n",
    "def deepseek_rag_pipeline(query):\n",
    "    retrieved_context = retrieve_context(query)\n",
    "\n",
    "    # Construct prompt with retrieved knowledge\n",
    "    full_prompt = f\"\"\"\n",
    "    You are an AI assistant. Use the retrieved knowledge below to answer accurately:\n",
    "\n",
    "    Retrieved Context:\n",
    "    {retrieved_context}\n",
    "\n",
    "    Question: {query}\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate response using DeepSeek\n",
    "    response = deepseek_generate(full_prompt)\n",
    "    \n",
    "    return response\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    query = \"do you know what is UMCH?\"\n",
    "    response = deepseek_rag_pipeline(query)\n",
    "    print(response)\n",
    "\n",
    "if __name__ ==\"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
